# vLLM WebUI

A modern web interface for managing and interacting with vLLM (Very Large Language Model) servers. Supports both GPU and CPU modes, with special optimizations for macOS Apple Silicon.

![vLLM WebUI Interface](assets/vllm-webui.png)

## ğŸ“ Project Structure

```
vllm-webui/
â”œâ”€â”€ app.py                       # Main FastAPI backend application
â”œâ”€â”€ run.py                       # Backend server launcher
â”œâ”€â”€ index.html                   # Main HTML interface
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ LICENSE                      # MIT License
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ Containerfile                # Podman/Docker container definition ğŸ³
â”œâ”€â”€ .containerignore             # Container build exclusions
â”œâ”€â”€ Makefile                     # Quick commands for container management
â”œâ”€â”€ CONTAINER-QUICKSTART.md      # Container quick start guide ğŸ³
â”œâ”€â”€ README-CONTAINER.md          # Complete container documentation ğŸ³
â”‚
â”œâ”€â”€ static/                      # Frontend assets
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ style.css           # Main stylesheet
â”‚   â””â”€â”€ js/
â”‚       â””â”€â”€ app.js              # Frontend JavaScript
â”‚
â”œâ”€â”€ scripts/                    # Utility scripts
â”‚   â”œâ”€â”€ run_cpu.sh             # Start vLLM in CPU mode (macOS compatible)
â”‚   â”œâ”€â”€ start.sh               # General start script
â”‚   â”œâ”€â”€ install.sh             # Installation script
â”‚   â”œâ”€â”€ verify_setup.py        # Setup verification
â”‚   â”œâ”€â”€ build_container.sh     # Build Podman container ğŸ³
â”‚   â”œâ”€â”€ run_container.sh       # Run Podman container ğŸ³
â”‚   â”œâ”€â”€ stop_container.sh      # Stop Podman container ğŸ³
â”‚   â”œâ”€â”€ test_container.sh      # Test container setup ğŸ³
â”‚   â””â”€â”€ docker-compose-up.sh   # Docker Compose wrapper ğŸ³
â”‚
â”œâ”€â”€ config/                     # Configuration files
â”‚   â”œâ”€â”€ vllm_cpu.env           # CPU mode environment variables
â”‚   â””â”€â”€ example_configs.json   # Example configurations
â”‚
â””â”€â”€ docs/                       # Documentation
    â”œâ”€â”€ QUICKSTART.md           # Quick start guide
    â”œâ”€â”€ MACOS_CPU_GUIDE.md      # macOS CPU setup guide
    â”œâ”€â”€ CPU_MODELS_QUICKSTART.md # CPU-optimized models guide
    â”œâ”€â”€ GATED_MODELS_GUIDE.md   # Guide for accessing Llama, Gemma, etc.
    â”œâ”€â”€ CHAT_TEMPLATES.md       # Model-specific chat templates
    â”œâ”€â”€ TROUBLESHOOTING.md      # Common issues and solutions
    â”œâ”€â”€ FEATURES.md             # Feature documentation
    â”œâ”€â”€ PERFORMANCE_METRICS.md  # Performance metrics
    â””â”€â”€ QUICK_REFERENCE.md      # Command reference
```

## ğŸš€ Quick Start

### ğŸ³ Option 1: Container (Easiest for macOS) **RECOMMENDED**

For macOS users, the container provides the easiest setup with everything pre-configured:

```bash
# 1. Build the container (one-time, ~15-30 min)
./scripts/build_container.sh

# 2. Run the container
./scripts/run_container.sh

# 3. Open http://localhost:7860
```

**âœ¨ Benefits:**
- âœ… No complex installation
- âœ… Pre-built vLLM optimized for CPU
- âœ… Isolated environment
- âœ… Works out of the box

**ğŸ“– See [CONTAINER-QUICKSTART.md](CONTAINER-QUICKSTART.md)** for detailed instructions.

---

### ğŸ’» Option 2: Local Installation

For local development or if you prefer not to use containers:

#### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

#### 2. Install vLLM

```bash
# For macOS/CPU mode
pip install vllm
```

#### 3. Start the WebUI

```bash
python run.py
```

Then open http://localhost:7860 in your browser.

#### 4. Start vLLM Server

**Option A: Using the WebUI**
- Select CPU or GPU mode
- Click "Start Server"

**Option B: Using the script (macOS/CPU)**
```bash
./scripts/run_cpu.sh
```

## ğŸ’» macOS Apple Silicon Support

For macOS users, vLLM runs in CPU mode. See [docs/MACOS_CPU_GUIDE.md](docs/MACOS_CPU_GUIDE.md) for detailed setup.

**Quick CPU Mode Setup:**
```bash
# Edit CPU configuration
nano config/vllm_cpu.env

# Run vLLM
./scripts/run_cpu.sh
```

## âœ¨ Features

- **Server Management**: Start/stop vLLM servers from the UI
- **Chat Interface**: Interactive chat with streaming responses
- **Smart Chat Templates**: Automatic model-specific template detection (Nov 2025) ğŸ†•
- **Performance Metrics**: Real-time token counts and generation speed
- **Model Support**: Pre-configured popular models + custom model support
- **Gated Model Access**: Built-in HuggingFace token support for Llama, Gemma, etc.
- **CPU & GPU Modes**: Automatic detection and configuration
- **macOS Optimized**: Special support for Apple Silicon
- **Benchmarking**: GuideLLM integration for performance testing
- **Resizable Panels**: Customizable layout
- **Command Preview**: See exact commands before execution

## ğŸ“– Documentation

### Getting Started
- **[Container Quick Start](CONTAINER-QUICKSTART.md)** ğŸ³ - Easiest way for macOS users (RECOMMENDED)
- **[Container Full Guide](README-CONTAINER.md)** - Complete container documentation
- **[Container Workflow](CONTAINER-WORKFLOW.md)** - Step-by-step container workflow
- **[Quick Start Guide](docs/QUICKSTART.md)** - Get up and running in minutes
- [macOS CPU Setup](docs/MACOS_CPU_GUIDE.md) - Apple Silicon optimization guide
- [CPU Models Quickstart](docs/CPU_MODELS_QUICKSTART.md) - Best models for CPU

### Model Configuration
- **[Gated Models Guide (Llama, Gemma)](docs/GATED_MODELS_GUIDE.md)** â­ - Access restricted models
- **[Chat Templates Explained](docs/CHAT_TEMPLATES.md)** ğŸ†• - Model-specific templates

### Reference
- [Feature Overview](docs/FEATURES.md) - Complete feature list
- [Performance Metrics](docs/PERFORMANCE_METRICS.md) - Benchmarking and metrics
- [Command Reference](docs/QUICK_REFERENCE.md) - Command cheat sheet
- [Troubleshooting](docs/TROUBLESHOOTING.md) - Common issues and solutions

## ğŸ”§ Configuration

### CPU Mode (macOS)

Edit `config/vllm_cpu.env`:
```bash
export VLLM_CPU_KVCACHE_SPACE=40
export VLLM_CPU_OMP_THREADS_BIND=auto
```

### Supported Models

**CPU-Optimized Models (Recommended for macOS):**
- **TinyLlama/TinyLlama-1.1B-Chat-v1.0** (default) - Fast, no token required
- **meta-llama/Llama-3.2-1B** - Latest Llama, requires HF token (gated)
- **google/gemma-2-2b** - High quality, requires HF token (gated)
- facebook/opt-125m - Tiny test model

**Larger Models (Slow on CPU, better on GPU):**
- meta-llama/Llama-2-7b-chat-hf (requires HF token)
- mistralai/Mistral-7B-Instruct-v0.2
- Custom models via text input

**ğŸ“Œ Note**: Gated models (Llama, Gemma) require a HuggingFace token. See [Gated Models Guide](docs/GATED_MODELS_GUIDE.md) for setup.

## ğŸ› ï¸ Development

### Project Structure

- **Backend**: FastAPI (`app.py`)
- **Frontend**: Vanilla JavaScript (`static/js/app.js`)
- **Styling**: Custom CSS (`static/css/style.css`)
- **Scripts**: Bash scripts in `scripts/`
- **Config**: Environment files in `config/`

### Running in Development

```bash
# Start backend with auto-reload
uvicorn app:app --reload --port 7860

# Or use the run script
python run.py
```

## ğŸ“ License

MIT License - See [LICENSE](LICENSE) file for details

## ğŸ¤ Contributing

Contributions welcome! Please feel free to submit issues and pull requests.

## ğŸ”— Links

- [vLLM Official Documentation](https://docs.vllm.ai/)
- [vLLM CPU Mode Guide](https://docs.vllm.ai/en/stable/getting_started/installation/cpu.html)
- [vLLM GitHub](https://github.com/vllm-project/vllm)

## ğŸ†˜ Troubleshooting

### macOS Segmentation Fault

Use CPU mode with proper environment variables. See [docs/MACOS_CPU_GUIDE.md](docs/MACOS_CPU_GUIDE.md).

### Server Won't Start

1. Check if vLLM is installed: `python -c "import vllm; print(vllm.__version__)"`
2. Check port availability: `lsof -i :8000`
3. Review server logs in the WebUI

### Chat Not Streaming

Check browser console (F12) for errors and ensure the server is running.

---

Made with â¤ï¸ for the vLLM community
