---
# vLLM Playground Development OpenShift/Kubernetes Deployment
# Ultra-minimal container for manual vLLM installation and testing
# Image: quay.io/rh_ee_micyang/vllm-playground:0.1
#
# This deployment creates an ultra-minimal development environment (based on UBI9-minimal)
# Container includes Python 3.11, build tools (gcc, git, vim), and application code (~800MB-1.2GB)
# Users must install Python dependencies after deployment:
# - vLLM (or vllm-cpu-only)
# - PyTorch
# - WebUI dependencies (from requirements.txt)
#
# Benefits:
# - Smallest possible image size for fast deployment
# - Maximum flexibility - choose your own vLLM version and backend
# - Install only what you need when you need it
#
# Quick Deploy:
#   oc apply -f openshift-deployment-dev.yaml
#
# With custom namespace:
#   oc apply -f openshift-deployment-dev.yaml -n your-namespace
#
# To access the container shell:
#   oc exec -it deployment/vllm-playground-dev -n vllm-playground-dev -- /bin/bash
#

---
# Namespace (optional - comment out if using existing namespace)
apiVersion: v1
kind: Namespace
metadata:
  name: vllm-playground-dev
  labels:
    name: vllm-playground-dev
    app: vllm-playground-dev

---
# Optional: Secret for HuggingFace Token (for gated models like Llama, Gemma)
# Uncomment and add your token to use gated models
# Get token from: https://huggingface.co/settings/tokens
#
# apiVersion: v1
# kind: Secret
# metadata:
#   name: hf-token
#   namespace: vllm-playground-dev
# type: Opaque
# stringData:
#   HF_TOKEN: "hf_your_token_here"

---
# Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-playground-dev
  namespace: vllm-playground-dev
  labels:
    app: vllm-playground-dev
    app.kubernetes.io/name: vllm-playground-dev
    app.kubernetes.io/component: development
    app.kubernetes.io/version: "0.1"
spec:
  replicas: 1
  strategy:
    type: Recreate  # Changed to Recreate for development environment
  selector:
    matchLabels:
      app: vllm-playground-dev
  template:
    metadata:
      labels:
        app: vllm-playground-dev
        app.kubernetes.io/name: vllm-playground-dev
      annotations:
        # Force pod restart on config change
        checksum/config: "1"
    spec:
      # Security Context
      # Note: runAsUser and fsGroup are omitted to allow OpenShift to automatically
      # assign UIDs from the namespace's allocated range
      securityContext:
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault

      containers:
      - name: vllm-playground-dev
        image: quay.io/rh_ee_micyang/vllm-playground:0.1
        imagePullPolicy: Always

        # Command to keep container running for interactive access
        # Use uid_entrypoint.sh wrapper to fix "I have no name" issue with arbitrary UIDs
        # Override the default entrypoint to allow manual installation
        command: ["/home/vllm/uid_entrypoint.sh"]
        args: ["/bin/bash", "-c", "while true; do sleep 3600; done"]

        # Ports
        ports:
        - name: webui
          containerPort: 7860
          protocol: TCP
        - name: vllm-api
          containerPort: 8000
          protocol: TCP

        # Environment Variables
        env:
        - name: WEBUI_PORT
          value: "7860"
        - name: VLLM_PORT
          value: "8000"
        - name: PYTHONUNBUFFERED
          value: "1"
        # HuggingFace cache directory - use writable mounted volume
        - name: HF_HOME
          value: "/workspace/.cache/huggingface"
        - name: TRANSFORMERS_CACHE
          value: "/workspace/.cache/huggingface/transformers"
        - name: HF_HUB_CACHE
          value: "/workspace/.cache/huggingface/hub"
        # Set work directory for user operations
        - name: WORKDIR
          value: "/workspace"

        # Uncomment to use HuggingFace token from secret
        # - name: HF_TOKEN
        #   valueFrom:
        #     secretKeyRef:
        #       name: hf-token
        #       key: HF_TOKEN

        # Resource Limits and Requests
        # Configured for GPU-enabled development
        # Uncomment the GPU lines below to enable GPU access
        resources:
          requests:
            cpu: "4"
            memory: "8Gi"
            # Uncomment for GPU access:
            nvidia.com/gpu: "1"
          limits:
            cpu: "8"
            memory: "16Gi"
            # Uncomment for GPU access:
            nvidia.com/gpu: "1"

        # Health Checks
        # Using simple exec probe since this is a development container
        livenessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - "exit 0"
          initialDelaySeconds: 30
          periodSeconds: 60
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5

        readinessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - "exit 0"
          initialDelaySeconds: 10
          periodSeconds: 30
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 3

        # Security Context
        securityContext:
          runAsNonRoot: true
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: false  # Allow writes for development
          capabilities:
            drop:
            - ALL

        # Volume Mounts for persistent workspace
        volumeMounts:
        - name: workspace
          mountPath: /workspace

      # Volumes for persistent workspace
      volumes:
      - name: workspace
        persistentVolumeClaim:
          claimName: vllm-dev-workspace

      restartPolicy: Always

      # Optional: Node selector for GPU nodes
      # Uncomment to schedule on GPU nodes only
      # nodeSelector:
      #   nvidia.com/gpu.present: "true"
      # Or use OpenShift GPU label:
      #   node-role.kubernetes.io/gpu: ""

      # Optional: Tolerations for GPU nodes
      # Uncomment if GPU nodes have taints
      # tolerations:
      # - key: nvidia.com/gpu
      #   operator: Exists
      #   effect: NoSchedule

---
# Service for WebUI (optional - for when user starts the web interface)
apiVersion: v1
kind: Service
metadata:
  name: vllm-playground-dev-webui
  namespace: vllm-playground-dev
  labels:
    app: vllm-playground-dev
    service: webui
spec:
  type: ClusterIP
  ports:
  - name: webui
    port: 7860
    targetPort: 7860
    protocol: TCP
  selector:
    app: vllm-playground-dev
  sessionAffinity: None

---
# Service for vLLM API (optional - for when user starts vLLM)
apiVersion: v1
kind: Service
metadata:
  name: vllm-playground-dev-api
  namespace: vllm-playground-dev
  labels:
    app: vllm-playground-dev
    service: api
spec:
  type: ClusterIP
  ports:
  - name: vllm-api
    port: 8000
    targetPort: 8000
    protocol: TCP
  selector:
    app: vllm-playground-dev
  sessionAffinity: None

---
# OpenShift Route for WebUI (remove if using plain Kubernetes)
# Uncomment when ready to expose the web interface
#
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: vllm-playground-dev-webui
  namespace: vllm-playground-dev
  labels:
    app: vllm-playground-dev
  annotations:
    haproxy.router.openshift.io/timeout: 5m
spec:
  to:
    kind: Service
    name: vllm-playground-dev-webui
    weight: 100
  port:
    targetPort: webui
  tls:
    termination: edge
    insecureEdgeTerminationPolicy: Redirect
  wildcardPolicy: None

---
# OpenShift Route for vLLM API (remove if using plain Kubernetes)
# Uncomment when ready to expose the vLLM API
#
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: vllm-playground-dev-api
  namespace: vllm-playground-dev
  labels:
    app: vllm-playground-dev
  annotations:
    haproxy.router.openshift.io/timeout: 5m
spec:
  to:
    kind: Service
    name: vllm-playground-dev-api
    weight: 100
  port:
    targetPort: vllm-api
  tls:
    termination: edge
    insecureEdgeTerminationPolicy: Redirect
  wildcardPolicy: None

---
# PersistentVolumeClaim for development workspace
# 80GB minimum for:
# - Model downloads (models can be 5-50GB each)
# - Python packages and dependencies
# - Logs and temporary files
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: vllm-dev-workspace
  namespace: vllm-playground-dev
  labels:
    app: vllm-playground-dev
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 80Gi
  # Optional: specify storage class for faster storage if available
  # storageClassName: fast-ssd

---
# ConfigMap for development environment documentation
apiVersion: v1
kind: ConfigMap
metadata:
  name: vllm-dev-guide
  namespace: vllm-playground-dev
  labels:
    app: vllm-playground-dev
data:
  README.md: |
    # vLLM Playground Development Environment

    This is an ultra-minimal development container (UBI9-minimal based).
    Container includes Python 3.11, build tools (gcc, git, vim), and application code.
    You must install Python dependencies (vLLM, PyTorch, WebUI libs) manually.

    ## Getting Started

    1. Access the container shell:
       ```bash
       oc exec -it deployment/vllm-playground-dev -n vllm-playground-dev -- /bin/bash
       ```

    2. Verify build tools (pre-installed):
       ```bash
       # Build tools (gcc, gcc-c++, python3.11-devel, git, vim) are pre-installed
       gcc --version
       git --version
       vim --version
       ```

    3. Upgrade pip:
       ```bash
       pip3 install --upgrade pip setuptools wheel --user
       ```

    4. Install vLLM (choose based on your hardware):
       ```bash
       # For CUDA GPU:
       pip3 install --user torch torchvision --index-url https://download.pytorch.org/whl/cu121
       pip3 install --user vllm

       # For CPU only:
       pip3 install --user torch torchvision --index-url https://download.pytorch.org/whl/cpu
       pip3 install --user vllm-cpu-only

       # For ROCm (AMD GPU):
       pip3 install --user torch torchvision --index-url https://download.pytorch.org/whl/rocm6.1
       pip3 install --user vllm
       ```

    5. Install WebUI dependencies:
       ```bash
       pip3 install --user -r /home/vllm/vllm-playground/requirements.txt
       ```

       ```bash
       ```

    ## Workspace Layout

    - `/workspace` - Your persistent workspace (80GB)
    - `/workspace/.cache/huggingface` - Model cache directory
    - `/home/vllm/vllm-playground` - Application code directory
    - All your work should be done in `/workspace` to ensure persistence

    ## Starting the WebUI Application

    After installing all dependencies:
    ```bash
    cd /home/vllm/vllm-playground
    python3 app.py
    ```

    ## Downloading Models

    Models will be automatically cached in `/workspace/.cache/huggingface/hub/`

    Example:
    ```python
    from transformers import AutoModelForCausalLM, AutoTokenizer

    model_name = "meta-llama/Llama-2-7b-hf"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    ```

    ## Running vLLM Server

    ```bash
    vllm serve <model-name> --host 0.0.0.0 --port 8000
    ```

    ## Tips

    - Monitor disk usage: `df -h /workspace`
    - Check model cache: `du -sh /workspace/.cache/huggingface/hub/*`
    - Clean up old models if needed to free space

    ## Exposing Services

    Once you have services running, uncomment the Route sections in the deployment
    file to expose them externally.
