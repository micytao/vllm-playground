---
# vLLM Playground OpenShift/Kubernetes Deployment
# Image: quay.io/rh_ee_micyang/vllm-playground-cuda:0.1
#
# Quick Deploy:
#   oc apply -f openshift-deployment.yaml
#
# With custom namespace:
#   oc apply -f openshift-deployment.yaml -n your-namespace
#

---
# Namespace (optional - comment out if using existing namespace)
apiVersion: v1
kind: Namespace
metadata:
  name: vllm-playground
  labels:
    name: vllm-playground
    app: vllm-playground

---
# Optional: Secret for HuggingFace Token (for gated models like Llama, Gemma)
# Uncomment and add your token to use gated models
# Get token from: https://huggingface.co/settings/tokens
#
# apiVersion: v1
# kind: Secret
# metadata:
#   name: hf-token
#   namespace: vllm-playground
# type: Opaque
# stringData:
#   HF_TOKEN: "hf_your_token_here"

---
# Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-playground
  namespace: vllm-playground
  labels:
    app: vllm-playground
    app.kubernetes.io/name: vllm-playground
    app.kubernetes.io/component: webui
    app.kubernetes.io/version: "0.1"
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: vllm-playground
  template:
    metadata:
      labels:
        app: vllm-playground
        app.kubernetes.io/name: vllm-playground
      annotations:
        # Force pod restart on config change
        checksum/config: "1"
    spec:
      # Security Context
      # Note: runAsUser and fsGroup are omitted to allow OpenShift to automatically
      # assign UIDs from the namespace's allocated range
      securityContext:
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault

      containers:
      - name: vllm-playground
        image: quay.io/rh_ee_micyang/vllm-playground-cuda:0.1
        imagePullPolicy: Always

        # Ports
        ports:
        - name: webui
          containerPort: 7860
          protocol: TCP
        - name: vllm-api
          containerPort: 8000
          protocol: TCP

        # Environment Variables
        env:
        - name: WEBUI_PORT
          value: "7860"
        - name: VLLM_PORT
          value: "8000"
        - name: VLLM_TARGET_DEVICE
          value: "cuda"
        - name: CUDA_VISIBLE_DEVICES
          value: "all"
        - name: NVIDIA_VISIBLE_DEVICES
          value: "all"
        - name: NVIDIA_DRIVER_CAPABILITIES
          value: "compute,utility"
        - name: PYTHONUNBUFFERED
          value: "1"
        # HuggingFace cache directory - use writable mounted volume
        - name: HF_HOME
          value: "/home/vllm/.cache/huggingface"
        - name: TRANSFORMERS_CACHE
          value: "/home/vllm/.cache/huggingface/transformers"
        - name: HF_HUB_CACHE
          value: "/home/vllm/.cache/huggingface/hub"

        # Uncomment to use HuggingFace token from secret
        # - name: HF_TOKEN
        #   valueFrom:
        #     secretKeyRef:
        #       name: hf-token
        #       key: HF_TOKEN

        # Resource Limits and Requests
        resources:
          requests:
            cpu: "4"
            memory: "8Gi"
            nvidia.com/gpu: "1"
          limits:
            cpu: "8"
            memory: "16Gi"
            nvidia.com/gpu: "1"

        # Health Checks
        livenessProbe:
          httpGet:
            path: /api/status
            port: 7860
            scheme: HTTP
          initialDelaySeconds: 120
          periodSeconds: 30
          timeoutSeconds: 10
          successThreshold: 1
          failureThreshold: 3

        readinessProbe:
          httpGet:
            path: /api/status
            port: 7860
            scheme: HTTP
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 3

        startupProbe:
          httpGet:
            path: /api/status
            port: 7860
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 12

        # Security Context
        securityContext:
          runAsNonRoot: true
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: false
          capabilities:
            drop:
            - ALL

        # Volume Mounts for persistent model cache
        volumeMounts:
        - name: model-cache
          mountPath: /home/vllm/.cache

      # Volumes for persistent model cache
      volumes:
      - name: model-cache
        persistentVolumeClaim:
          claimName: vllm-model-cache

      restartPolicy: Always

      # Optional: Node selector for GPU nodes
      # nodeSelector:
      #   nvidia.com/gpu.present: "true"

      # Optional: Tolerations for GPU nodes
      # tolerations:
      # - key: nvidia.com/gpu
      #   operator: Exists
      #   effect: NoSchedule

---
# Service for WebUI
apiVersion: v1
kind: Service
metadata:
  name: vllm-playground-webui
  namespace: vllm-playground
  labels:
    app: vllm-playground
    service: webui
spec:
  type: ClusterIP
  ports:
  - name: webui
    port: 7860
    targetPort: 7860
    protocol: TCP
  selector:
    app: vllm-playground
  sessionAffinity: None

---
# Service for vLLM API
apiVersion: v1
kind: Service
metadata:
  name: vllm-playground-api
  namespace: vllm-playground
  labels:
    app: vllm-playground
    service: api
spec:
  type: ClusterIP
  ports:
  - name: vllm-api
    port: 8000
    targetPort: 8000
    protocol: TCP
  selector:
    app: vllm-playground
  sessionAffinity: None

---
# OpenShift Route for WebUI (remove if using plain Kubernetes)
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: vllm-playground-webui
  namespace: vllm-playground
  labels:
    app: vllm-playground
  annotations:
    haproxy.router.openshift.io/timeout: 5m
spec:
  to:
    kind: Service
    name: vllm-playground-webui
    weight: 100
  port:
    targetPort: webui
  tls:
    termination: edge
    insecureEdgeTerminationPolicy: Redirect
  wildcardPolicy: None

---
# OpenShift Route for vLLM API (remove if using plain Kubernetes)
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: vllm-playground-api
  namespace: vllm-playground
  labels:
    app: vllm-playground
  annotations:
    haproxy.router.openshift.io/timeout: 5m
spec:
  to:
    kind: Service
    name: vllm-playground-api
    weight: 100
  port:
    targetPort: vllm-api
  tls:
    termination: edge
    insecureEdgeTerminationPolicy: Redirect
  wildcardPolicy: None

---
# PersistentVolumeClaim for model cache
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: vllm-model-cache
  namespace: vllm-playground
  labels:
    app: vllm-playground
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi
  # Optional: specify storage class
  # storageClassName: fast-ssd

---
# Optional: HorizontalPodAutoscaler
# Uncomment to enable auto-scaling based on CPU/Memory
#
# apiVersion: autoscaling/v2
# kind: HorizontalPodAutoscaler
# metadata:
#   name: vllm-playground-hpa
#   namespace: vllm-playground
# spec:
#   scaleTargetRef:
#     apiVersion: apps/v1
#     kind: Deployment
#     name: vllm-playground
#   minReplicas: 1
#   maxReplicas: 3
#   metrics:
#   - type: Resource
#     resource:
#       name: cpu
#       target:
#         type: Utilization
#         averageUtilization: 70
#   - type: Resource
#     resource:
#       name: memory
#       target:
#         type: Utilization
#         averageUtilization: 80

---
# Optional: NetworkPolicy for enhanced security
# Uncomment to restrict network access
#
# apiVersion: networking.k8s.io/v1
# kind: NetworkPolicy
# metadata:
#   name: vllm-playground-netpol
#   namespace: vllm-playground
# spec:
#   podSelector:
#     matchLabels:
#       app: vllm-playground
#   policyTypes:
#   - Ingress
#   - Egress
#   ingress:
#   - from:
#     - namespaceSelector:
#         matchLabels:
#           name: openshift-ingress
#     ports:
#     - protocol: TCP
#       port: 7860
#     - protocol: TCP
#       port: 8000
#   egress:
#   - to:
#     - namespaceSelector: {}
#     ports:
#     - protocol: TCP
#       port: 443  # For downloading models
#     - protocol: TCP
#       port: 80
