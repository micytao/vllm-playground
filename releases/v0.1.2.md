# üåè vLLM Playground v0.1.2 - ModelScope Integration & i18n Improvements

**Release Date:** January 19, 2026

This release brings **ModelScope support** for users in China who cannot access HuggingFace, comprehensive **Chinese language translations**, and several UI/UX improvements including chat export functionality.

![vLLM Playground Chinese Support](https://raw.githubusercontent.com/micytao/vllm-playground/main/assets/vllm-playground-chinese-support.png)

*ModelScope integration with full Chinese language support - optimized for users in China.*

---

## üåü Highlights

### üîÆ ModelScope Integration (China Region Support)

For users in China where HuggingFace is not accessible, vLLM Playground now supports **ModelScope (È≠îÊê≠Á§æÂå∫)** as an alternative model source.

#### Features
- **New Model Source Option** - Toggle between HuggingFace, ModelScope, and Local
- **Curated Model List** - Pre-configured models optimized for ModelScope:
  - üñ•Ô∏è CPU-Friendly: Qwen 2.5 0.5B Instruct
  - üéÆ GPU-Optimized: Qwen 2.5 3B/Coder 7B, DeepSeek R1 Distill Qwen 7B
- **ModelScope Token Support** - Optional token for private/gated models
- **Auto-Detection** - Checks if `modelscope` SDK is installed
- **Installation Hints** - UI guidance when SDK is not installed

#### Usage
```bash
# Install ModelScope SDK
pip install modelscope>=1.18.1

# Models are cached to ~/.cache/modelscope/hub/
```

When ModelScope is selected, vLLM Playground automatically sets:
- `VLLM_USE_MODELSCOPE=1`
- `MODELSCOPE_SDK_TOKEN` (if provided)

---

### üåê i18n Chinese Translations

Comprehensive Chinese language support across the application:

#### Newly Translated Sections
- **Response Metrics Panel** - ÂìçÂ∫îÊåáÊ†á
  - Prompt Tokens (ÊèêÁ§∫ËØç‰ª§Áâå)
  - Completion Tokens (Ë°•ÂÖ®‰ª§Áâå)
  - Total Tokens (ÊÄª‰ª§ÁâåÊï∞)
  - Time Taken (ËÄóÊó∂)
  - Tokens/sec (‰ª§Áâå/Áßí)
  - Avg Prompt/Generation Throughput (Âπ≥ÂùáÂêûÂêêÈáè)
  - GPU KV Cache Usage (GPU KV ÁºìÂ≠ò‰ΩøÁî®Áéá)
  - Prefix Cache Hit Rate (ÂâçÁºÄÁºìÂ≠òÂëΩ‰∏≠Áéá)

- **GuideLLM Benchmark** - ÊÄßËÉΩÂü∫ÂáÜÊµãËØï
  - Benchmark Method (Âü∫ÂáÜÊµãËØïÊñπÊ≥ï)
  - Built-in/GuideLLM options
  - Configuration labels
  - Command Preview (ÂëΩ‰ª§È¢ÑËßà)

- **Server Configuration** - ÊúçÂä°Âô®ÈÖçÁΩÆ
  - HuggingFace/ModelScope Token labels
  - CPU settings (KV Cache, Thread Binding)
  - Data Type, Max Model Length
  - Checkboxes (Trust Remote Code, Prefix Caching, Tool Calling)

- **Logs Panel** - ÊúçÂä°Âô®Êó•Âøó
  - Auto-scroll, Save, Clear buttons

---

### üí¨ Chat Improvements

#### Export Chat
- **New Export Button** - Save your conversation history
- **Multiple Formats** - Export as JSON or text

#### Clear Chat Confirmation
- **Confirmation Modal** - Prevents accidental chat clearing
- **Safe UX** - Must confirm before deleting conversation

---

## üêõ Bug Fixes

### Windows Unicode Fix
- **Fixed** `UnicodeDecodeError` when reading HTML files on Windows
- **Solution**: UTF-8 encoding with latin-1 fallback
- Applied to both root and package `app.py`

### Collapsed Sidebar UI Fixes
- **Fixed** MCP nav icon not displaying properly when sidebar collapsed
- **Fixed** Nav badge (server count) overlapping icon when collapsed
- **Fixed** Status dot being cut off by version number when collapsed
- **Solution**: Proper `display: none` for hidden elements, consistent icon sizing

---

## üìã Detailed Changes

### New Features
- ‚ú® ModelScope as model source (China region support)
- ‚ú® ModelScope token configuration
- ‚ú® ModelScope SDK detection and installation hints
- ‚ú® Export chat functionality
- ‚ú® Clear chat confirmation modal
- ‚ú® Chinese translations for Response Metrics
- ‚ú® Chinese translations for GuideLLM Benchmark
- ‚ú® Chinese translations for Server Configuration labels

### Improvements
- üîß Better link visibility in help text (brighter blue color)
- üîß Consistent nav icon sizing (22px)
- üîß Cleaner collapsed sidebar state
- üîß Package includes i18n locale files

### Bug Fixes
- üêõ Windows Unicode decoding issue in app.py
- üêõ MCP icon shrinking when sidebar collapsed
- üêõ Nav badge visibility in collapsed state
- üêõ Status dot layout affected by version text

---

## üîÑ Migration Notes

### From v0.1.1

No breaking changes. All existing configurations continue to work.

**New optional dependency:**
```bash
# For ModelScope support (optional)
pip install modelscope>=1.18.1
```

**New features available:**
- ModelScope model source in Server Configuration
- Export button in Chat toolbar
- Chinese translations (switch language in UI)

---

## üì¶ Installation

```bash
# Upgrade from v0.1.1
pip install --upgrade vllm-playground

# Fresh install
pip install vllm-playground

# With ModelScope support
pip install vllm-playground
pip install modelscope>=1.18.1

# Start the playground
vllm-playground
```

---

## üá®üá≥ ModelScope Quick Start

### For Users in China

1. **Install ModelScope SDK**:
   ```bash
   pip install modelscope>=1.18.1
   ```

2. **Select ModelScope** as Model Source in Server Configuration

3. **Choose a model** from the curated list or enter a custom model ID

4. **Optional**: Add your ModelScope token for private models
   - Get token from: https://www.modelscope.cn/my/myaccesstoken

5. **Start the server** - Models will download from ModelScope

### Model Cache Location
- **Linux/macOS**: `~/.cache/modelscope/hub/`
- **Windows**: `C:\Users\<username>\.cache\modelscope\hub\`

---

## üôè Acknowledgments

- The [ModelScope](https://www.modelscope.cn/) team for providing model hosting for China
- The [vLLM Project](https://github.com/vllm-project/vllm) team for ModelScope integration
- Community contributors for Windows compatibility fixes
- Users who requested China region support

---

## üîó Links

- **GitHub**: https://github.com/micytao/vllm-playground
- **PyPI**: https://pypi.org/project/vllm-playground/
- **ModelScope**: https://www.modelscope.cn/

---

Made with ‚ù§Ô∏è for the vLLM community worldwide üåç
