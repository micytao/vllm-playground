# vLLM Playground v0.1.3 - Multi-Accelerators & Claude Code

**Release Date:** January 22, 2026

This release brings **multi-accelerator support** (NVIDIA, AMD ROCm, TPU), **Claude Code integration** for using open-source models as a coding assistant, **vLLM-Metal support** for Apple Silicon GPU acceleration, and a major **codebase restructure** for easier development.

![Claude Code Demo](https://raw.githubusercontent.com/micytao/vllm-playground/main/assets/vllm-playground-claude-code.gif)

*Run Claude Code with open-source models served by vLLM - your private, local coding assistant.*

---

## Highlights

### Multi-Accelerators Support

vLLM Playground now supports multiple GPU accelerators beyond NVIDIA:

![Multi-GPU Selection](https://raw.githubusercontent.com/micytao/vllm-playground/main/assets/vllm-playground-multi-GPUs.png)

#### Features
- **NVIDIA CUDA** - Default GPU support (unchanged)
- **AMD ROCm** - Full support for AMD GPUs with ROCm
- **Google Cloud TPU** - TPU support for cloud deployments

#### Auto-Detection
- Automatically detects available accelerators via:
  - `nvidia-smi` for NVIDIA GPUs
  - `amd-smi` for AMD GPUs
  - `tpu-info` for Google TPUs
- Kubernetes detection for `amd.com/gpu` and `google.com/tpu` resources

#### CLI Pull Commands
```bash
vllm-playground pull --nvidia    # NVIDIA GPU image (default)
vllm-playground pull --amd       # AMD ROCm image
vllm-playground pull --tpu       # TPU image
vllm-playground pull --cpu       # CPU-only image
vllm-playground pull --all       # All images
```

---

### Claude Code Integration

Use vLLM to serve open-source models as a backend for [Claude Code](https://docs.anthropic.com/en/docs/claude-code):

![Claude Code](https://raw.githubusercontent.com/micytao/vllm-playground/main/assets/vllm-playground-claude-code.png)

#### Features
- **Embedded Web Terminal** - ttyd-powered terminal for Claude Code TUI
- **WebSocket Proxy** - Works in cloud deployments (browser connects to server, server proxies to ttyd)
- **One-Click Setup** - Start vLLM, then launch Claude Code from the sidebar
- **Recommended Models** - Tips for models with 65K+ context and tool calling

#### Requirements
- vLLM v0.12.0+ (for Anthropic Messages API)
- Model with native 65K+ context and tool calling support
- [ttyd](https://github.com/tsl0922/ttyd) installed for web terminal

#### Installation
```bash
# macOS
brew install ttyd

# Ubuntu/Debian
apt install ttyd
```

#### Recommended Model
```bash
meta-llama/Llama-3.1-8B-Instruct
--max-model-len 65536 --enable-auto-tool-choice --tool-call-parser llama3_json
```

---

### vLLM-Metal Support (Apple Silicon)

Apple Silicon GPU acceleration via [vllm-metal](https://github.com/vllm-project/vllm-metal):

![Metal GPU Mode](https://raw.githubusercontent.com/micytao/vllm-playground/main/assets/vllm-metal-support.png)

#### Features
- **Metal Compute Mode** - New option alongside CPU and GPU
- **Custom Virtual Environment** - Point to your vllm-metal installation
- **Multi-Method Detection** - Python import → pip list → uv pip list

#### Setup
1. Install vllm-metal:
   ```bash
   curl -fsSL https://raw.githubusercontent.com/vllm-project/vllm-metal/main/install.sh | bash
   ```
2. Configure playground:
   - Run Mode: Subprocess
   - Compute Mode: Metal
   - Venv Path: `~/.venv-vllm-metal`

See [macOS Metal Guide](../docs/MACOS_METAL_GUIDE.md) for details.

---

### Single Source Restructure

**Breaking Change:** All source code now lives in `vllm_playground/` only.

#### What Changed
- Removed ~32,000 lines of duplicate root-level files
- Root `app.py`, `container_manager.py`, `index.html`, `static/`, etc. are gone
- Added `CONTRIBUTING.md` with development guidelines
- Added `scripts/verify_structure.py` for validation

#### Migration
- **For users**: No action needed, `pip install` works as before
- **For developers**: Edit files in `vllm_playground/`, not root
- **Running locally**: `python run.py` (unchanged)

---

### Container Image Update

Default container image updated to **`vllm/vllm-openai:v0.12.0`**:

| Previous | New |
|----------|-----|
| `vllm/vllm-openai:v0.11.0` | `vllm/vllm-openai:v0.12.0` |

**Why v0.12.0?**
- Required for Claude Code (Anthropic Messages API support)
- Pinned version ensures reproducible deployments

Update with:
```bash
vllm-playground pull
```

---

## Detailed Changes

### New Features
- Multi-accelerator support (NVIDIA, AMD ROCm, TPU)
- Claude Code integration with ttyd web terminal
- vLLM-Metal support for Apple Silicon
- Custom virtual environment path (`venv_path`)
- `compute_mode` configuration (cpu/gpu/metal)
- Single source restructure
- Multi-method vLLM detection

### Improvements
- Container image updated to v0.12.0
- CLI pull flags for different accelerators
- Kubernetes resource detection
- WebSocket proxy for cloud deployments
- Improved UI messaging for vLLM states

### Bug Fixes
- sys.path pollution in run.py
- Command preview matching Metal execution

### Breaking Changes
- Root-level source files removed

---

## Installation

```bash
# Upgrade from v0.1.2
pip install --upgrade vllm-playground

# Fresh install
pip install vllm-playground

# With all optional dependencies
pip install vllm-playground[all]

# Start the playground
vllm-playground
```

### Pre-Release Testing
```bash
# Install release candidate
pip install --pre vllm-playground
```

---

## New Dependencies

### System Packages
- **ttyd** - Required for Claude Code terminal
  - macOS: `brew install ttyd`
  - Ubuntu: `apt install ttyd`

### Python Packages
No new Python dependencies.

---

## Documentation

- [Custom venv Guide](../docs/CUSTOM_VENV_GUIDE.md) - Using custom vLLM installations
- [macOS Metal Guide](../docs/MACOS_METAL_GUIDE.md) - Apple Silicon GPU acceleration
- [CONTRIBUTING.md](../CONTRIBUTING.md) - Development guidelines

---

## Acknowledgments

- The [vLLM Project](https://github.com/vllm-project/vllm) team
- The [vllm-metal](https://github.com/vllm-project/vllm-metal) contributors
- The [Claude Code](https://docs.anthropic.com/en/docs/claude-code) team at Anthropic
- The [ttyd](https://github.com/tsl0922/ttyd) project
- Community contributors and testers

---

## Links

- **GitHub**: https://github.com/micytao/vllm-playground
- **PyPI**: https://pypi.org/project/vllm-playground/
- **vllm-metal**: https://github.com/vllm-project/vllm-metal
- **Claude Code**: https://docs.anthropic.com/en/docs/claude-code

---

Made with ❤️ for the vLLM community
