# vLLM CPU Container for RHEL 9 / vllm-playground
# Modified for RHEL 9 / Podman compatibility - uses COPY instead of bind mounts
# Optimized for production deployment with vllm-playground
#
# Supported platforms:
#   - linux/amd64 (x86_64)
#   - linux/arm64 (aarch64)
#
# Build arguments:
#   max_jobs=4 (default) - Number of parallel compile jobs (lower = more stable)
#   PYTHON_VERSION=3.12 (default)|3.11|3.10
#   VLLM_CPU_DISABLE_AVX512=false (default)|true - Disable AVX512 for older CPUs
#   VLLM_CPU_AVX512BF16=false (default)|true
#   VLLM_CPU_AVX512VNNI=false (default)|true
#   VLLM_CPU_AMXBF16=false (default)|true
#
# Build command:
#   podman build -f Containerfile.cpu -t vllm-cpu:latest .
#   podman build -f Containerfile.cpu --build-arg max_jobs=2 -t vllm-cpu:latest .

######################### BASE IMAGE (Common) #########################
FROM ubuntu:22.04 AS base-common

WORKDIR /workspace/

ARG PYTHON_VERSION=3.12
ARG PIP_EXTRA_INDEX_URL="https://download.pytorch.org/whl/cpu"

# Install system dependencies
RUN --mount=type=cache,target=/var/cache/apt,sharing=locked \
    --mount=type=cache,target=/var/lib/apt,sharing=locked \
    apt-get update -y \
    && apt-get install -y --no-install-recommends \
        sudo ccache git curl wget ca-certificates \
        gcc-12 g++-12 libtcmalloc-minimal4 libnuma-dev \
        ffmpeg libsm6 libxext6 libgl1 jq lsof \
    && update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-12 10 --slave /usr/bin/g++ g++ /usr/bin/g++-12 \
    && curl -LsSf https://astral.sh/uv/install.sh | sh

# Compiler configuration
ENV CC=/usr/bin/gcc-12 CXX=/usr/bin/g++-12
ENV CCACHE_DIR=/root/.cache/ccache
ENV CMAKE_CXX_COMPILER_LAUNCHER=ccache

# Python environment setup
ENV PATH="/root/.local/bin:$PATH"
ENV VIRTUAL_ENV="/opt/venv"
ENV UV_PYTHON_INSTALL_DIR=/opt/uv/python
RUN uv venv --python ${PYTHON_VERSION} --seed ${VIRTUAL_ENV}
ENV PATH="$VIRTUAL_ENV/bin:$PATH"

# UV package manager configuration
ENV UV_HTTP_TIMEOUT=500
ENV PIP_EXTRA_INDEX_URL=${PIP_EXTRA_INDEX_URL}
ENV UV_EXTRA_INDEX_URL=${PIP_EXTRA_INDEX_URL}
ENV UV_INDEX_STRATEGY="unsafe-best-match"
ENV UV_LINK_MODE="copy"

# Install Python dependencies
COPY requirements/ /tmp/requirements/
RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install --upgrade pip && \
    uv pip install -r /tmp/requirements/cpu.txt && \
    rm -rf /tmp/requirements

ARG TARGETARCH
ENV TARGETARCH=${TARGETARCH}

######################### BASE IMAGE (x86_64) #########################
FROM base-common AS base-amd64
ENV LD_PRELOAD="/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4:/opt/venv/lib/libiomp5.so"

######################### BASE IMAGE (arm64) #########################
FROM base-common AS base-arm64
ENV LD_PRELOAD="/usr/lib/aarch64-linux-gnu/libtcmalloc_minimal.so.4"

######################### BASE IMAGE (Architecture-specific) #########################
FROM base-${TARGETARCH} AS base
RUN echo 'ulimit -c 0' >> ~/.bashrc

######################### BUILD STAGE #########################
FROM base AS vllm-build

# Build configuration
ARG max_jobs=8
ENV MAX_JOBS=${max_jobs}

ARG GIT_REPO_CHECK=0
ARG VLLM_CPU_DISABLE_AVX512=0
ENV VLLM_CPU_DISABLE_AVX512=${VLLM_CPU_DISABLE_AVX512}

ARG VLLM_CPU_AVX512BF16=0
ENV VLLM_CPU_AVX512BF16=${VLLM_CPU_AVX512BF16}

ARG VLLM_CPU_AVX512VNNI=0
ENV VLLM_CPU_AVX512VNNI=${VLLM_CPU_AVX512VNNI}

ARG VLLM_CPU_AMXBF16=0
ENV VLLM_CPU_AMXBF16=${VLLM_CPU_AMXBF16}

WORKDIR /workspace/vllm

# Install build dependencies
COPY requirements/ requirements/
RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install -r requirements/cpu-build.txt

# Copy source code (including .git for version detection)
COPY . .

# Optional repository check
RUN if [ "$GIT_REPO_CHECK" != 0 ]; then bash tools/check_repo.sh ; fi

# Display build configuration
RUN echo "=== vLLM Build Configuration ===" && \
    echo "Python: $(python3 --version)" && \
    echo "GCC: $(gcc --version | head -1)" && \
    echo "CPU: $(lscpu | grep 'Model name' || echo 'N/A')" && \
    echo "Cores: $(nproc)" && \
    echo "MAX_JOBS: $MAX_JOBS" && \
    echo "VLLM_TARGET_DEVICE: cpu" && \
    echo "================================"

# Build vLLM wheel for CPU
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=cache,target=/root/.cache/ccache \
    --mount=type=cache,target=/workspace/vllm/.deps,sharing=locked \
    VLLM_TARGET_DEVICE=cpu \
    CMAKE_BUILD_TYPE=Release \
    VERBOSE=1 \
    python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38

######################### PRODUCTION IMAGE #########################
FROM base AS vllm-openai

WORKDIR /workspace/

# Install vLLM from built wheel
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=cache,target=/root/.cache/ccache \
    --mount=type=bind,from=vllm-build,src=/workspace/vllm/dist,target=dist \
    uv pip install dist/*.whl

# Create startup script for vllm-playground integration
COPY --chmod=755 <<'SCRIPT' /workspace/start_vllm.sh
#!/bin/bash
set -e

echo "=== vLLM CPU Service Container ==="
echo "Python version: $(python3 --version)"
echo "Virtual environment: $VIRTUAL_ENV"
echo "Python location: $(which python3)"

# Check if vLLM is installed
echo "Checking vLLM installation..."
if ! python3 -c "import vllm" 2>/dev/null; then
    echo "ERROR: vLLM is not installed or cannot be imported!"
    exit 1
fi

echo "vLLM version: $(python3 -c 'import vllm; print(vllm.__version__)' 2>/dev/null || echo 'unknown')"
echo ""

# Force CPU mode (critical for CPU-only deployments)
export VLLM_TARGET_DEVICE=${VLLM_TARGET_DEVICE:-cpu}
export VLLM_PLATFORM=${VLLM_PLATFORM:-cpu}
export CUDA_VISIBLE_DEVICES=""

# CPU performance tuning (configurable from UI)
export VLLM_CPU_KVCACHE_SPACE=${VLLM_CPU_KVCACHE_SPACE:-4}
export VLLM_CPU_OMP_THREADS_BIND=${VLLM_CPU_OMP_THREADS_BIND:-auto}

# HuggingFace authentication (for gated models)
[ -n "$HF_TOKEN" ] && export HUGGING_FACE_HUB_TOKEN=$HF_TOKEN

# Cache directories (if not already set by container manager)
export HF_HOME=${HF_HOME:-/root/.cache/huggingface}
export HUGGINGFACE_HUB_CACHE=${HUGGINGFACE_HUB_CACHE:-/root/.cache/huggingface/hub}

# Configurable parameters (passed by vllm-playground orchestrator)
MODEL=${VLLM_MODEL:-"TinyLlama/TinyLlama-1.1B-Chat-v1.0"}
HOST=${VLLM_HOST:-"0.0.0.0"}
PORT=${VLLM_PORT:-8000}
DTYPE=${VLLM_DTYPE:-"bfloat16"}
MAX_MODEL_LEN=${VLLM_MAX_MODEL_LEN:-""}
TRUST_REMOTE_CODE=${VLLM_TRUST_REMOTE_CODE:-""}
CHAT_TEMPLATE=${VLLM_CHAT_TEMPLATE:-""}
DISABLE_LOG_STATS=${VLLM_DISABLE_LOG_STATS:-""}
ENABLE_PREFIX_CACHING=${VLLM_ENABLE_PREFIX_CACHING:-""}
LOAD_FORMAT=${VLLM_LOAD_FORMAT:-"auto"}

echo "=========================================="
echo "vLLM CPU Service Configuration"
echo "=========================================="
echo "Model: $MODEL"
echo "Host: $HOST"
echo "Port: $PORT"
echo "Dtype: $DTYPE"
echo "Platform: $VLLM_PLATFORM"
echo "Device: $VLLM_TARGET_DEVICE"
echo "CPU KV Cache Space: $VLLM_CPU_KVCACHE_SPACE GB"
echo "CPU OMP Threads Bind: $VLLM_CPU_OMP_THREADS_BIND"
echo "Load Format: $LOAD_FORMAT"
echo "CUDA_VISIBLE_DEVICES: $CUDA_VISIBLE_DEVICES"
[ -n "$MAX_MODEL_LEN" ] && echo "Max Model Length: $MAX_MODEL_LEN"
[ "$TRUST_REMOTE_CODE" = "true" ] && echo "Trust Remote Code: enabled"
[ "$DISABLE_LOG_STATS" = "true" ] && echo "Log Stats: disabled"
[ "$ENABLE_PREFIX_CACHING" = "true" ] && echo "Prefix Caching: enabled"
[ -n "$HF_TOKEN" ] && echo "HuggingFace Token: configured"
echo "=========================================="
echo ""

# Build vLLM command with parameters
CMD="python3 -m vllm.entrypoints.openai.api_server"
CMD="$CMD --model $MODEL"
CMD="$CMD --host $HOST"
CMD="$CMD --port $PORT"
CMD="$CMD --dtype $DTYPE"

# Add optional parameters
[ -n "$MAX_MODEL_LEN" ] && CMD="$CMD --max-model-len $MAX_MODEL_LEN"
[ "$TRUST_REMOTE_CODE" = "true" ] && CMD="$CMD --trust-remote-code"
[ -n "$CHAT_TEMPLATE" ] && CMD="$CMD --chat-template $CHAT_TEMPLATE"
[ "$DISABLE_LOG_STATS" = "true" ] && CMD="$CMD --disable-log-stats"
[ "$ENABLE_PREFIX_CACHING" = "true" ] && CMD="$CMD --enable-prefix-caching"

echo "Starting vLLM with command:"
echo "$CMD"
echo ""
echo "Container logs available via: podman logs vllm-service"
echo ""

# Run vLLM with error handling
set +e
$CMD
EXIT_CODE=$?
echo ""
echo "vLLM exited with code: $EXIT_CODE"
exit $EXIT_CODE
SCRIPT

# Expose vLLM API port
EXPOSE 8000

# Health check - verify vLLM API is responding
HEALTHCHECK --interval=30s --timeout=10s --start-period=180s --retries=3 \
    CMD curl -f http://localhost:8000/health || curl -f http://localhost:8000/v1/models || exit 1

# Use CMD instead of ENTRYPOINT for easier override if needed
CMD ["/bin/bash", "/workspace/start_vllm.sh"]
