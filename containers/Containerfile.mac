# vLLM Service Container for macOS (CPU-only)
# Based on Fedora Minimal (100% free, Red Hat family, ~50% smaller than standard Fedora)
# This container runs ONLY vLLM OpenAI-compatible API server
# For use with vLLM-Playground container orchestrator

FROM registry.fedoraproject.org/fedora-minimal:latest

USER 0

# Set environment variables
ENV PYTHONUNBUFFERED=1 \
    VLLM_TARGET_DEVICE=cpu \
    VLLM_BUILD_WITH_CUDA=0 \
    VLLM_LOGGING_LEVEL=DEBUG \
    HOME=/home/vllm \
    PYTHON_VERSION=3.12.7
# Install Python 3.12 and build dependencies for vLLM CPU backend
RUN microdnf install -y \
    python3.12 \
    python3.12-devel \
    gcc \
    gcc-c++ \
    make \
    git \
    numactl-devel \
    openssl \
    curl \
    which \
    findutils \
    && microdnf clean all && \
    ln -sf /usr/bin/python3.12 /usr/bin/python3 && \
    python3.12 -m ensurepip --upgrade && \
    ln -sf /usr/local/bin/pip3.12 /usr/bin/pip3

# Create non-root user
RUN useradd -m -u 1001 -s /bin/bash vllm

# Switch to non-root user
USER 1001

# Set working directory
WORKDIR ${HOME}

# Create virtual environment and build vLLM from source for CPU
# CPU backend MUST be built with VLLM_TARGET_DEVICE=cpu set during build
# Using regular install (not editable) so we can clean up source files
RUN python3 -m venv vllm_env && \
    source vllm_env/bin/activate && \
    git clone --single-branch --branch releases/v0.11.0 https://github.com/vllm-project/vllm.git && \
    cd vllm && \
    pip install --no-cache-dir --upgrade pip setuptools wheel && \
    pip install --no-cache-dir torch torchvision --index-url https://download.pytorch.org/whl/cpu && \
    export VLLM_TARGET_DEVICE=cpu && \
    export VLLM_BUILD_WITH_CUDA=0 && \
    pip install --no-cache-dir -v . && \
    echo "=== Verifying vLLM installation ===" && \
    python3 -c "import vllm; print(f'vLLM version: {vllm.__version__}')" && \
    python3 -c "from vllm import SamplingParams; print('SamplingParams import: OK')" && \
    echo "=== vLLM installation verified ===" && \
    cd ${HOME} && \
    rm -rf vllm && \
    echo "=== Source files cleaned up ==="
# Expose vLLM API port
EXPOSE 8000

# Create entrypoint script that activates venv and passes through all arguments
# This allows CLI arguments like the official vLLM image
RUN cat > ${HOME}/entrypoint.sh << 'EOF'
#!/bin/bash
set -e

# Activate virtual environment
source ${HOME}/vllm_env/bin/activate

# Force CPU mode (critical for macOS)
export VLLM_TARGET_DEVICE=${VLLM_TARGET_DEVICE:-cpu}
export VLLM_PLATFORM=${VLLM_PLATFORM:-cpu}
export CUDA_VISIBLE_DEVICES=""

# CPU performance tuning
export VLLM_CPU_KVCACHE_SPACE=${VLLM_CPU_KVCACHE_SPACE:-4}
export VLLM_CPU_OMP_THREADS_BIND=${VLLM_CPU_OMP_THREADS_BIND:-auto}

echo "=========================================="
echo "vLLM Service Container (CPU Mode)"
echo "=========================================="
echo "vLLM version: $(python3 -c 'import vllm; print(vllm.__version__)' 2>/dev/null || echo 'unknown')"
echo "Platform: $VLLM_PLATFORM"
echo "Device: $VLLM_TARGET_DEVICE"
echo "CPU KV Cache Space: $VLLM_CPU_KVCACHE_SPACE GB"
echo "=========================================="
echo ""
echo "Starting vLLM with arguments: $@"
echo ""

# Execute the command with all passed arguments
exec python3 -m vllm.entrypoints.openai.api_server "$@"
EOF

RUN chmod +x ${HOME}/entrypoint.sh

# ENTRYPOINT runs our script which activates venv and runs vLLM
ENTRYPOINT ["/home/vllm/entrypoint.sh"]

# Default CMD - these are the default arguments if none are provided
# Can be completely overridden by passing arguments to docker/podman run
CMD ["--model", "TinyLlama/TinyLlama-1.1B-Chat-v1.0", "--host", "0.0.0.0", "--port", "8000", "--dtype", "bfloat16"]

# Health check - verify vLLM API is responding
HEALTHCHECK --interval=30s --timeout=10s --start-period=180s --retries=3 \
    CMD curl -f http://localhost:8000/health || curl -f http://localhost:8000/v1/models || exit 1
